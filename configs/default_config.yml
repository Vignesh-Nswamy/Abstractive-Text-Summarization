data:
    train_path: 'data/cnn_dailymail/train_shards/*.tfrecord'
    val_path: 'data/cnn_dailymail/val.tfrecord'
    test_path: 'data/cnn_dailymail/test.tfrecord'
    shuffle_buffer_size: 256
    batch_size: 32
    # max_len should be less than 512
    max_len: 512

# Do not change this portion if you want to use training checkpoints
transformer:
    encoder:
        is_bert: True
        bert_hub_url: 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'
        finetune_bert: False
        # If is_bert is True, nothing below is considered
        # The below is for a custom encoder
        d_model: 768
        num_layers: 4
        max_sequence_length: 512
        dropout_rate: 0.4
        intermediate_size: 512
        num_attention_heads: 8
        attention_dropout_rate: 0.3

    decoder:
        d_model: 768
        vocab_size: target_vocab_size,
        num_layers: 4
        max_sequence_length: 512
        dropout_rate: 0.4
        intermediate_size: 512
        num_attention_heads: 8
        attention_dropout_rate: 0.3